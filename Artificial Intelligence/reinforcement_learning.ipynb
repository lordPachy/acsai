{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# MDP"
      ],
      "metadata": {
        "id": "a1uKJOCCgT0s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E2LulhdIu_pr"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from collections import defaultdict\n",
        "def vector_add(a, b):\n",
        "    \"\"\"Component-wise addition of two vectors.\"\"\"\n",
        "    if not (a and b):\n",
        "        return a or b\n",
        "    if hasattr(a, '__iter__') and hasattr(b, '__iter__'):\n",
        "        assert len(a) == len(b)\n",
        "        return list(map(vector_add, a, b))\n",
        "    else:\n",
        "        return a + b\n",
        "\n",
        "def isnumber(x):\n",
        "    \"\"\"Is x a number?\"\"\"\n",
        "    return hasattr(x, '__int__')\n",
        "\n",
        "def print_table(table, header=None, sep='', numfmt='{}'):\n",
        "    \"\"\"Print a list of lists as a table, so that columns line up nicely.\n",
        "    header, if specified, will be printed as the first row.\n",
        "    numfmt is the format for all numbers; you might want e.g. '{:.2f}'.\n",
        "    (If you want different formats in different columns,\n",
        "    don't use print_table.) sep is the separator between columns.\"\"\"\n",
        "    justs = ['rjust' if isnumber(x) else 'ljust' for x in table[0]]\n",
        "\n",
        "    if header:\n",
        "        table.insert(0, header)\n",
        "\n",
        "    table = [[numfmt.format(x) if isnumber(x)\n",
        "                else \"###\" if x==None\n",
        "                else x for x in row]\n",
        "             for row in table]\n",
        "    sizes = list(\n",
        "        map(lambda seq: max(map(len, seq)),\n",
        "            list(zip(*[map(str, row) for row in table]))))\n",
        "\n",
        "    for row in table:\n",
        "        print(sep.join(getattr(\n",
        "            str(x), j)(size) for (j, size, x) in zip(justs, sizes, row)))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GridMDP:\n",
        "    \"\"\"\n",
        "    A Markov Decision Process on a two-dimensional grid.\n",
        "    Attributes:\n",
        "        grid (list of lists): Reward grid, where None indicates obstacles.\n",
        "        terminals (set): Terminal states.\n",
        "        init (tuple): Initial state.\n",
        "        gamma (float): Discount factor (0 < gamma <= 1).\n",
        "        rows (int): Number of rows in the grid.\n",
        "        cols (int): Number of columns in the grid.\n",
        "        orientations (tuple): Valid directions as unit vectors: (east, north, west, south).\n",
        "        turns (tuple): Turn directions: (left, right).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, grid, terminals, init=(2, 0), gamma=0.99):\n",
        "        # Reverse grid for bottom-to-top indexing\n",
        "        self.grid = grid[::-1]\n",
        "        self.rows = len(grid)\n",
        "        self.cols = len(grid[0])\n",
        "\n",
        "        # Extract states, reward, and validate input\n",
        "        self.states = set()\n",
        "        self.reward = {}\n",
        "        for y in range(self.rows):\n",
        "            for x in range(self.cols):\n",
        "                if self.grid[y][x] is not None:\n",
        "                    self.states.add((x, y))\n",
        "                    self.reward[(x, y)] = self.grid[y][x]\n",
        "\n",
        "        if init not in self.states:\n",
        "            raise ValueError(\"Invalid initial state:\", init)\n",
        "        if any(t not in self.states for t in terminals):\n",
        "            raise ValueError(\"Invalid terminal states:\", terminals)\n",
        "\n",
        "        self.terminals = terminals\n",
        "        self.init = init\n",
        "        self.gamma = gamma\n",
        "        self.orientations = EAST, NORTH, WEST, SOUTH = [(1, 0), (0, 1), (-1, 0), (0, -1)]\n",
        "        self.turns = LEFT, RIGHT = (+1, -1)\n",
        "        # Precompute transition probabilities for efficiency\n",
        "        self.transitions = {s: self._calculate_T(s) for s in self.states}\n",
        "\n",
        "\n",
        "\n",
        "    def _calculate_T(self, s):\n",
        "        \"\"\"\n",
        "        Calculate transition probabilities for all actions from a state.\n",
        "\n",
        "        Args:\n",
        "        state (tuple): Current state.\n",
        "\n",
        "         Returns:\n",
        "            dict: Mapping from action to list of (probability, next_state) pairs.\n",
        "        \"\"\"\n",
        "        transitions = {action: [(0.8, self._go(s, action))] for action in self.orientations}\n",
        "        for action in transitions:\n",
        "            transitions[action].append((0.1, self._go(s, self._turn_direction(action, -1))))\n",
        "            transitions[action].append((0.1, self._go(s, self._turn_direction(action, +1))))\n",
        "        return transitions\n",
        "\n",
        "    def _turn_direction(self, direction, turn):\n",
        "        \"\"\"\n",
        "        Turn the given direction by the specified amount.\n",
        "\n",
        "        Args:\n",
        "            direction (tuple): Current direction.\n",
        "            turn (int): direction to turn (left: -1, right: 1).\n",
        "\n",
        "        Returns:\n",
        "            tuple: New direction.\n",
        "        \"\"\"\n",
        "        index = self.orientations.index(direction)\n",
        "        return self.orientations[(index + turn) % len(self.orientations)]\n",
        "\n",
        "    def _go(self, state, direction):\n",
        "        \"\"\"\n",
        "        Move one step in the given direction, handling boundaries.\n",
        "\n",
        "        Args:\n",
        "            state (tuple): Current state.\n",
        "            direction (tuple): Direction to move.\n",
        "\n",
        "        Returns:\n",
        "            tuple: New state.\n",
        "        \"\"\"\n",
        "        new_state = tuple(vector_add(state, direction))\n",
        "        return new_state if new_state in self.states else state\n",
        "\n",
        "    def R(self, state):\n",
        "        \"\"\"\n",
        "        Get the reward for a state.\n",
        "\n",
        "        Args:\n",
        "            state (tuple): State.\n",
        "\n",
        "        Returns:\n",
        "            float: Reward.\n",
        "        \"\"\"\n",
        "        return self.reward[state]\n",
        "\n",
        "    def T(self, state, action):\n",
        "        \"\"\"\n",
        "        Get the transition probabilities for a state and action.\n",
        "\n",
        "        Args:\n",
        "            state (tuple): State.\n",
        "            action (tuple): Action.\n",
        "\n",
        "        Returns:\n",
        "            list: List of (probability, next_state) pairs.\n",
        "        \"\"\"\n",
        "        return self.transitions[state][action] if action else [(0.0, state)]\n",
        "\n",
        "\n",
        "    def actions(self, state):\n",
        "        \"\"\"\n",
        "        Get the available actions in a state (always oriented actions).\n",
        "\n",
        "        Args:\n",
        "            state (tuple): State.\n",
        "\n",
        "        Returns:\n",
        "            list: List of actions (possible directions).\n",
        "        \"\"\"\n",
        "        if state in self.terminals:\n",
        "            return [None]\n",
        "        else:\n",
        "            return self.orientations\n",
        "\n",
        "    def to_grid(self, mapping):\n",
        "        \"\"\"\n",
        "        Convert a mapping from (x, y) to values into a grid representation.\n",
        "\n",
        "        Args:\n",
        "            mapping (dict): Mapping from (x, y) to values.\n",
        "\n",
        "        Returns:\n",
        "            list of lists: Grid representation.\n",
        "        \"\"\"\n",
        "        return list(reversed([[mapping.get((x, y), None) for x in range(self.cols)]\n",
        "                              for y in range(self.rows)]))\n",
        "\n",
        "    def to_arrows(self, policy):\n",
        "        \"\"\"\n",
        "        Convert a policy (mapping from state to action) into a grid showing corresponding arrow directions.\n",
        "\n",
        "        Args:\n",
        "            policy (dict): Mapping from state to action.\n",
        "\n",
        "        Returns:\n",
        "            list of lists: Grid representation with arrows.\n",
        "        \"\"\"\n",
        "        chars = {(1, 0): \" > \", (0, 1): ' ∧ ', (-1, 0): ' < ', (0, -1): ' ∨ ', None: ' O '}\n",
        "        return self.to_grid({s: chars[a] for (s, a) in policy.items()})\n"
      ],
      "metadata": {
        "id": "sCaUNyqqyMOG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MDP Initialization"
      ],
      "metadata": {
        "id": "m5qLSMG3gYR4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "grid = [\n",
        "    [None, +1.0, None, None, None],\n",
        "    [None, -.01, -.01, -1.0, None],\n",
        "    [None, -.01, -.01, None, None],\n",
        "    [None, -.01, -.01, None, None],\n",
        "    [None, None, -.01, None, None]\n",
        "]\n",
        "terminals = [(1, 4), (3, 3)]\n",
        "maze = GridMDP(grid, terminals)"
      ],
      "metadata": {
        "id": "IHdXkIjQ5ZrD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pi = {(1,2):(0,1), (2, 1): (0,1), (1, 1): (0,1), (2, 0): (0,1), (1, 4): None, (2, 3): (-1,0), (3, 3): None, (2, 2): (0,1), (1, 3): (0,1)}\n",
        "print(maze.states)\n",
        "print_table(maze.to_arrows(pi))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ccSZL2Ev4tIG",
        "outputId": "114b19ee-1676-4a55-d54c-8a342fdff6e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{(1, 2), (2, 1), (1, 1), (2, 0), (1, 4), (2, 3), (3, 3), (2, 2), (1, 3)}\n",
            "### O #########\n",
            "### ∧  <  O ###\n",
            "### ∧  ∧ ######\n",
            "### ∧  ∧ ######\n",
            "###### ∧ ######\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def run_single_trial(agent_program, mdp):\n",
        "    import random\n",
        "\n",
        "    def take_single_action(mdp,s , a):\n",
        "        return random.choice([ps[1] for ps  in mdp.T(s, a)])\n",
        "\n",
        "    current_state = mdp.init\n",
        "    while True:\n",
        "        current_reward = mdp.R(current_state)\n",
        "        percept = (current_state, current_reward)\n",
        "        next_action = agent_program(percept)\n",
        "        if next_action is None:\n",
        "            break\n",
        "        current_state = take_single_action(mdp, current_state, next_action)\n",
        "    return"
      ],
      "metadata": {
        "id": "IX1WxCgW4hGA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Direct Evaluation /Monte Carlo Estimation"
      ],
      "metadata": {
        "id": "VcgQkLUzgcHv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Passive_MC_Agent:\n",
        "    \"\"\"\n",
        "    Passive (non-learning) agent that uses direct utility estimation\n",
        "    on a given MDP and policy.\n",
        "\n",
        "    Attributes:\n",
        "        pi (dict): Mapping from states to actions (policy).\n",
        "        mdp (sequential_decision_environment): The MDP instance.\n",
        "        U (dict): Utility estimates for each state.\n",
        "        s (tuple, optional): Current state.\n",
        "        a (tuple, optional): Last action taken.\n",
        "        s_history (list): History of visited states.\n",
        "        r_history (list): History of received rewards.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, pi, mdp):\n",
        "        self.pi = pi\n",
        "        self.mdp = mdp\n",
        "        self.V = {}\n",
        "        self.reset_history()\n",
        "\n",
        "    def __call__(self, percept):\n",
        "        \"\"\"\n",
        "        Acts according to the policy and updates history.\n",
        "\n",
        "        Args:\n",
        "            percept (tuple): (state, reward) pair.\n",
        "\n",
        "        Returns:\n",
        "            tuple: The action chosen according to the policy.\n",
        "        \"\"\"\n",
        "        s, r = percept\n",
        "        self.s_history.append(s)\n",
        "        self.r_history.append(r)\n",
        "\n",
        "        if s in self.mdp.terminals:\n",
        "            self.s = self.a = None\n",
        "        else:\n",
        "            self.s, self.a = s, self.pi[s]\n",
        "        return self.a\n",
        "\n",
        "    def reset_history(self):\n",
        "        \"\"\"\n",
        "        Resets the agent's internal history.\n",
        "        \"\"\"\n",
        "        self.s = None\n",
        "        self.a = None\n",
        "        self.s_history = []\n",
        "        self.r_history = []\n",
        "\n",
        "    def estimate_V(self):\n",
        "        \"\"\"\n",
        "        Estimates utilities based on the current history.\n",
        "\n",
        "        Raises:\n",
        "            AssertionError: If the MDP is not in a terminal state.\n",
        "        \"\"\"\n",
        "        assert self.a is None, \"MDP is not in a terminal state\"\n",
        "        assert len(self.s_history) == len(self.r_history)\n",
        "\n",
        "        # Calculate utilities based on historical rewards\n",
        "        V_temp = {s: [] for s in self.s_history}\n",
        "        for i, s in enumerate(self.s_history):\n",
        "            V_temp[s] += [sum(self.r_history[i:])]\n",
        "        V_temp = {k: sum(v) / max(len(v), 1) for k, v in V_temp.items()}\n",
        "\n",
        "        # Update existing utilities or add new ones\n",
        "        for state, value in V_temp.items():\n",
        "            if state in self.V:\n",
        "                self.V[state] = (self.V[state] + value) / 2\n",
        "            else:\n",
        "                self.V[state] = value\n",
        "\n",
        "        self.reset_history()\n",
        "        return self.V"
      ],
      "metadata": {
        "id": "-2-Ot85S0z2n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent = Passive_MC_Agent(pi, maze)\n",
        "for i in range(200):\n",
        "    run_single_trial(agent,maze)\n",
        "    agent.estimate_V()\n",
        "\n",
        "print('\\n'.join([str(k)+':'+str(v) for k, v in agent.V.items()]))"
      ],
      "metadata": {
        "id": "fDYvoLs75atc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28227ae6-eed3-4cee-daa3-0a9dbd0613e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2, 0):0.8311924226758116\n",
            "(2, 1):0.8541879940330394\n",
            "(1, 1):0.7818058646983982\n",
            "(1, 2):0.9157404999469299\n",
            "(2, 2):0.9126969606255535\n",
            "(2, 3):0.9122347235814126\n",
            "(1, 3):0.9590284764993509\n",
            "(1, 4):1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Policy Evaluation Agent"
      ],
      "metadata": {
        "id": "bUNfb7XlhBH5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def policy_evaluation(mdp, pi, V, k=20):\n",
        "    \"\"\"Return an updated utility mapping V from each state in the MDP to its\n",
        "    utility, using an approximation (modified policy iteration).\"\"\"\n",
        "    for i in range(k):\n",
        "        for s in mdp.states:\n",
        "            V[s] = mdp.R(s) + mdp.gamma*sum(p*V[si] for p, si in mdp.T(s, pi[s]))\n",
        "    return V\n",
        "\n",
        "\n",
        "\n",
        "class Passive_PE_Agent:\n",
        "    \"\"\"\n",
        "    [Figure 21.2]\n",
        "    Passive (non-learning) agent that uses adaptive dynamic programming\n",
        "    on a given MDP and policy.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    class ModelMDP(GridMDP):\n",
        "        \"\"\"Class for implementing modified Version of input MDP with\n",
        "        an editable transition model P and a custom function T.\"\"\"\n",
        "\n",
        "        def __init__(self, grid, terminals, init_state, gamma):\n",
        "            super().__init__(grid, terminals, init_state, gamma)\n",
        "            nested_dict = lambda: defaultdict(nested_dict)\n",
        "            self.P = nested_dict()\n",
        "\n",
        "        def T(self, s, a):\n",
        "            \"\"\"Return a list of tuples with probabilities for states\n",
        "            based on the learnt model P.\"\"\"\n",
        "            return [(prob, res) for (res, prob) in self.P[(s, a)].items()]\n",
        "\n",
        "    def __init__(self, pi, grid, terminals, init, gamma):\n",
        "        self.pi = pi\n",
        "        self.mdp = Passive_PE_Agent.ModelMDP(grid,terminals,init, gamma)\n",
        "        self.V = {}\n",
        "        self.Nsa = defaultdict(int)\n",
        "        self.Ns1_sa = defaultdict(int)\n",
        "        self.s = None\n",
        "        self.a = None\n",
        "        self.visited = set()  # keeping track of visited states\n",
        "\n",
        "    def __call__(self, percept):\n",
        "        s1, r1 = percept\n",
        "        mdp = self.mdp\n",
        "        R, P, pi = mdp.reward, mdp.P, self.pi\n",
        "        s, a, Nsa, Ns1_sa, V = self.s, self.a, self.Nsa, self.Ns1_sa, self.V\n",
        "\n",
        "        if s1 not in self.visited:  # Reward is only known for visited state.\n",
        "            V[s1] = R[s1] = r1\n",
        "            self.visited.add(s1)\n",
        "        if s is not None:\n",
        "            Nsa[(s, a)] += 1\n",
        "            Ns1_sa[(s1, s, a)] += 1\n",
        "            # for each t such that Ns′|sa [t, s, a] is nonzero\n",
        "            for t in [res for (res, state, act), freq in Ns1_sa.items()\n",
        "                      if (state, act) == (s, a) and freq != 0]:\n",
        "                P[(s, a)][t] = Ns1_sa[(t, s, a)] / Nsa[(s, a)]\n",
        "\n",
        "\n",
        "\n",
        "        self.V = policy_evaluation(mdp, pi, V)\n",
        "\n",
        "        self.Nsa, self.Ns1_sa = Nsa, Ns1_sa\n",
        "        if s1 in mdp.terminals:\n",
        "            self.s = self.a = None\n",
        "        else:\n",
        "            self.s, self.a = s1, pi[s1]\n",
        "        return self.a\n"
      ],
      "metadata": {
        "id": "qCfU5YoCEAvY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent = Passive_PE_Agent(pi,grid, terminals, (2, 0), 0.99)\n",
        "for i in range(300):\n",
        "    run_single_trial(agent, maze)\n",
        "\n",
        "print('\\n'.join([str(k)+':'+str(v) for k, v in agent.V.items()]))"
      ],
      "metadata": {
        "id": "FnSzNdSpItbu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7442625-1ebc-4f95-f90c-74830dc6508c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2, 0):0.6616057847250056\n",
            "(1, 2):0.7784911122836556\n",
            "(2, 1):0.7103487481949818\n",
            "(1, 1):0.7206762966373008\n",
            "(1, 4):1.0\n",
            "(2, 3):0.7761287028938784\n",
            "(3, 3):-1.0\n",
            "(2, 2):0.7515515169859974\n",
            "(1, 3):0.8562314565764992\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Temporal Difference Agent"
      ],
      "metadata": {
        "id": "unJFhR03hGZm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Passive_TD_Agent:\n",
        "    \"\"\"\n",
        "    Passive (non-learning) agent that uses temporal differences (TD) to learn\n",
        "    utility estimates for a given policy.\n",
        "\n",
        "    Attributes:\n",
        "        pi (dict): Mapping from states to actions (policy).\n",
        "        V (dict): Utility estimates for each state.\n",
        "        Ns (dict): Counts of state visits.\n",
        "        s (object, optional): Current state.\n",
        "        a (object, optional): Last action taken.\n",
        "        r (float, optional): Last reward received.\n",
        "        gamma (float): Discount factor.\n",
        "        terminals (set): Set of terminal states.\n",
        "        alpha (function): Learning rate function (optional).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, pi, mdp, alpha=None):\n",
        "        self.pi = pi\n",
        "        self.V = {s: 0. for s in mdp.states}\n",
        "        self.Ns = {s: 0 for s in mdp.states}\n",
        "        self.s = None\n",
        "        self.a = None\n",
        "        self.r = None\n",
        "        self.gamma = mdp.gamma\n",
        "        self.terminals = mdp.terminals\n",
        "        self.alpha = alpha or (lambda n: 1 / n)  # Default alpha if not provided\n",
        "\n",
        "    def __call__(self, percept):\n",
        "        \"\"\"\n",
        "        Acts according to the policy and updates utility estimates using TD.\n",
        "        \"\"\"\n",
        "        s1, r1 = percept\n",
        "        if not self.Ns[s1]:\n",
        "            self.V[s1] = r1\n",
        "        # Update utility for the previous state if applicable\n",
        "        if self.s is not None:\n",
        "            self.Ns[self.s] += 1\n",
        "            alpha = self.alpha(self.Ns[self.s])  # Calculate learning rate\n",
        "            self.V[self.s] += alpha * (self.r + self.gamma * self.V[s1] - self.V[self.s])\n",
        "\n",
        "        # Update internal state\n",
        "        if s1 in self.terminals:\n",
        "            self.s = self.a = self.r = None\n",
        "        else:\n",
        "            self.s, self.a, self.r = s1, self.pi[s1], r1\n",
        "\n",
        "        return self.a"
      ],
      "metadata": {
        "id": "G6ttsCdffT4F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent = Passive_TD_Agent(pi, maze, alpha=lambda n: 60./(59+n))\n",
        "for i in range(200):\n",
        "    run_single_trial(agent,maze)\n",
        "print('\\n'.join([str(k)+':'+str(v) for k, v in agent.V.items()]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bu3tbkSHfwQ8",
        "outputId": "7e136146-45a4-41f5-a875-3b781eee7b2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 2):0.7701631849562328\n",
            "(2, 1):0.6942453221137471\n",
            "(1, 1):0.7140717953025868\n",
            "(2, 0):0.654312831576855\n",
            "(1, 4):1.0\n",
            "(2, 3):0.7429620808410596\n",
            "(3, 3):0.0\n",
            "(2, 2):0.718734749664611\n",
            "(1, 3):0.8059206649199855\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q-Learning Agent"
      ],
      "metadata": {
        "id": "wIJmhF7ThWQP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class QLearningAgent:\n",
        "\n",
        "    def __init__(self, mdp, alpha=None):\n",
        "\n",
        "        self.gamma = mdp.gamma\n",
        "        self.terminals = mdp.terminals\n",
        "        self.all_act = mdp.orientations\n",
        "        self.Q = defaultdict(float)\n",
        "        self.Nsa = defaultdict(float)\n",
        "        self.s = None\n",
        "        self.a = None\n",
        "        self.r = None\n",
        "\n",
        "        self.alpha = alpha or (lambda n: 1 / n)  # Default alpha if not provided\n",
        "\n",
        "\n",
        "    def f(self, u):\n",
        "        \"\"\"Exploration function\"\"\"\n",
        "        return u\n",
        "\n",
        "    def actions_in_state(self, state):\n",
        "        \"\"\"Return actions possible in given state.\n",
        "        Useful for max and argmax.\"\"\"\n",
        "        if state in self.terminals:\n",
        "            return [None]\n",
        "        else:\n",
        "            return self.all_act\n",
        "\n",
        "    def __call__(self, percept):\n",
        "        s1, r1 = percept\n",
        "        Q, Nsa, s, a, r = self.Q, self.Nsa, self.s, self.a, self.r\n",
        "        alpha, gamma, terminals = self.alpha, self.gamma, self.terminals,\n",
        "        actions_in_state = self.actions_in_state\n",
        "\n",
        "        if s in terminals:\n",
        "            Q[s, None] = r1\n",
        "        if s is not None:\n",
        "            Nsa[s, a] += 1\n",
        "            Q[s, a] += alpha(Nsa[s, a]) * (r + gamma * max(Q[s1, a1]\n",
        "                                                           for a1 in actions_in_state(s1)) - Q[s, a])\n",
        "        if s in terminals:\n",
        "            self.s = self.a = self.r = None\n",
        "        else:\n",
        "            self.s, self.r = s1, r1\n",
        "            self.a = max(actions_in_state(s1), key=lambda a1: self.f(Q[s1, a1]))\n",
        "        return self.a"
      ],
      "metadata": {
        "id": "2CRvV6DTiJkz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q_agent = QLearningAgent(maze, alpha=lambda n: 60./(59+n))\n",
        "for i in range(200):\n",
        "    run_single_trial(q_agent,maze)\n",
        "print('\\n'.join([str(k)+':'+str(v) for k, v in agent.V.items()]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B8AafI4Li4p7",
        "outputId": "07613b12-407c-4c5d-e780-c06b59fa2a7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 2):0.7701631849562328\n",
            "(2, 1):0.6942453221137471\n",
            "(1, 1):0.7140717953025868\n",
            "(2, 0):0.654312831576855\n",
            "(1, 4):1.0\n",
            "(2, 3):0.7429620808410596\n",
            "(3, 3):0.0\n",
            "(2, 2):0.718734749664611\n",
            "(1, 3):0.8059206649199855\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nUkmR4exi-MX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}