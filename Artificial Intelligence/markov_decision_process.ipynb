{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3F5nZPvYzi-"
      },
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Z4j_BDGcQxU"
      },
      "outputs": [],
      "source": [
        "def vector_add(a, b):\n",
        "    \"\"\"Component-wise addition of two vectors.\"\"\"\n",
        "    if not (a and b):\n",
        "        return a or b\n",
        "    if hasattr(a, '__iter__') and hasattr(b, '__iter__'):\n",
        "        assert len(a) == len(b)\n",
        "        return list(map(vector_add, a, b))\n",
        "    else:\n",
        "        return a + b\n",
        "\n",
        "def isnumber(x):\n",
        "    \"\"\"Is x a number?\"\"\"\n",
        "    return hasattr(x, '__int__')\n",
        "\n",
        "def print_table(table, header=None, sep='', numfmt='{}'):\n",
        "    \"\"\"Print a list of lists as a table, so that columns line up nicely.\n",
        "    header, if specified, will be printed as the first row.\n",
        "    numfmt is the format for all numbers; you might want e.g. '{:.2f}'.\n",
        "    (If you want different formats in different columns,\n",
        "    don't use print_table.) sep is the separator between columns.\"\"\"\n",
        "    justs = ['rjust' if isnumber(x) else 'ljust' for x in table[0]]\n",
        "\n",
        "    if header:\n",
        "        table.insert(0, header)\n",
        "\n",
        "    table = [[numfmt.format(x) if isnumber(x)\n",
        "                else \"###\" if x==None\n",
        "                else x for x in row]\n",
        "             for row in table]\n",
        "    sizes = list(\n",
        "        map(lambda seq: max(map(len, seq)),\n",
        "            list(zip(*[map(str, row) for row in table]))))\n",
        "\n",
        "    for row in table:\n",
        "        print(sep.join(getattr(\n",
        "            str(x), j)(size) for (j, size, x) in zip(justs, sizes, row)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxOKwMlRY2eK"
      },
      "source": [
        "# MDP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jSphAKV3aZeU"
      },
      "outputs": [],
      "source": [
        "class GridMDP:\n",
        "    \"\"\"\n",
        "    A Markov Decision Process on a two-dimensional grid.\n",
        "    Attributes:\n",
        "        grid (list of lists): Reward grid, where None indicates obstacles.\n",
        "        terminals (set): Terminal states.\n",
        "        init (tuple): Initial state.\n",
        "        gamma (float): Discount factor (0 < gamma <= 1).\n",
        "        rows (int): Number of rows in the grid.\n",
        "        cols (int): Number of columns in the grid.\n",
        "        orientations (tuple): Valid directions as unit vectors: (east, north, west, south).\n",
        "        turns (tuple): Turn directions: (left, right).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, grid, terminals, init=(1, 1), gamma=0.99):\n",
        "        # Reverse grid for bottom-to-top indexing\n",
        "        self.grid = grid[::-1]\n",
        "        self.rows = len(grid)\n",
        "        self.cols = len(grid[0])\n",
        "\n",
        "        # Extract states, reward, and validate input\n",
        "        self.states = set()\n",
        "        self.reward = {}\n",
        "        for y in range(self.rows):\n",
        "            for x in range(self.cols):\n",
        "                if self.grid[y][x] is not None:\n",
        "                    self.states.add((x, y))\n",
        "                    self.reward[(x, y)] = self.grid[y][x]\n",
        "\n",
        "        if init not in self.states:\n",
        "            raise ValueError(\"Invalid initial state:\", init)\n",
        "        if any(t not in self.states for t in terminals):\n",
        "            raise ValueError(\"Invalid terminal states:\", terminals)\n",
        "\n",
        "        self.terminals = terminals\n",
        "        self.init = init\n",
        "        self.gamma = gamma\n",
        "        self.orientations = EAST, NORTH, WEST, SOUTH = [(1, 0), (0, 1), (-1, 0), (0, -1)]  \n",
        "        #the 4 variables are transparent to self.orientation, it is like performing\n",
        "        #self.orientations = [(1, 0), (0, 1), (-1, 0), (0, -1)]\n",
        "        #EAST, NORTH, WEST, SOUTH = [(1, 0), (0, 1), (-1, 0), (0, -1)]\n",
        "        self.turns = LEFT, RIGHT = (+1, -1)\n",
        "\n",
        "        # Precompute transition probabilities for efficiency\n",
        "        self.transitions = {s: self._calculate_T(s) for s in self.states}\n",
        "\n",
        "    def _calculate_T(self, s):\n",
        "        \"\"\"\n",
        "        Calculate transition probabilities for all actions from a state.\n",
        "\n",
        "        Args:\n",
        "        state (tuple): Current state.\n",
        "\n",
        "         Returns:\n",
        "            dict: Mapping from action to list of (probability, next_state) pairs.\n",
        "        \"\"\"\n",
        "        transitions = {action: [(0.8, self._go(s, action))]\n",
        "                   for action in self.orientations}\n",
        "        for action in transitions:\n",
        "            transitions[action].append((0.1, self._go(s, self._turn_direction(action, -1))))\n",
        "            transitions[action].append((0.1, self._go(s, self._turn_direction(action, +1))))\n",
        "        return transitions\n",
        "\n",
        "    def _turn_direction(self, direction, turn):\n",
        "        \"\"\"\n",
        "        Turn the given direction by the specified amount.\n",
        "\n",
        "        Args:\n",
        "            direction (tuple): Current direction.\n",
        "            turn (int): direction to turn (left: -1, right: 1).\n",
        "\n",
        "        Returns:\n",
        "            tuple: New direction.\n",
        "        \"\"\"\n",
        "        index = self.orientations.index(direction)\n",
        "        return self.orientations[(index + turn) % len(self.orientations)]\n",
        "\n",
        "    def _go(self, state, direction):\n",
        "        \"\"\"\n",
        "        Move one step in the given direction, handling boundaries.\n",
        "\n",
        "        Args:\n",
        "            state (tuple): Current state.\n",
        "            direction (tuple): Direction to move.\n",
        "\n",
        "        Returns:\n",
        "            tuple: New state.\n",
        "        \"\"\"\n",
        "        new_state = tuple(vector_add(state, direction))\n",
        "        return new_state if new_state in self.states else state\n",
        "\n",
        "    def R(self, state):\n",
        "        \"\"\"\n",
        "        Get the reward for a state.\n",
        "\n",
        "        Args:\n",
        "            state (tuple): State.\n",
        "\n",
        "        Returns:\n",
        "            float: Reward.\n",
        "        \"\"\"\n",
        "        return self.reward[state]\n",
        "\n",
        "    def T(self, state, action):\n",
        "        \"\"\"\n",
        "        Get the transition probabilities for a state and action.\n",
        "\n",
        "        Args:\n",
        "            state (tuple): State.\n",
        "            action (tuple): Action.\n",
        "\n",
        "        Returns:\n",
        "            list: List of (probability, next_state) pairs.\n",
        "        \"\"\"\n",
        "        return self.transitions[state][action] if action else [(0.0, state)]\n",
        "\n",
        "\n",
        "    def actions(self, state):\n",
        "        \"\"\"\n",
        "        Get the available actions in a state (always oriented actions).\n",
        "\n",
        "        Args:\n",
        "            state (tuple): State.\n",
        "\n",
        "        Returns:\n",
        "            list: List of actions (possible directions).\n",
        "        \"\"\"\n",
        "        if state in self.terminals:\n",
        "            return [None]\n",
        "        else:\n",
        "            return self.orientations\n",
        "\n",
        "    def to_grid(self, mapping):\n",
        "        \"\"\"\n",
        "        Convert a mapping from (x, y) to values into a grid representation.\n",
        "\n",
        "        Args:\n",
        "            mapping (dict): Mapping from (x, y) to values.\n",
        "\n",
        "        Returns:\n",
        "            list of lists: Grid representation.\n",
        "        \"\"\"\n",
        "        return list(reversed([[mapping.get((x, y), None) for x in range(self.cols)]\n",
        "                              for y in range(self.rows)]))\n",
        "\n",
        "    def to_arrows(self, policy):\n",
        "        \"\"\"\n",
        "        Convert a policy (mapping from state to action) into a grid showing corresponding arrow directions.\n",
        "\n",
        "        Args:\n",
        "            policy (dict): Mapping from state to action.\n",
        "\n",
        "        Returns:\n",
        "            list of lists: Grid representation with arrows.\n",
        "        \"\"\"\n",
        "        chars = {(1, 0): \" > \", (0, 1): ' ∧ ', (-1, 0): ' < ', (0, -1): ' ∨ ', None: ' G '}\n",
        "        return self.to_grid({s: chars[a] for (s, a) in policy.items()})\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97Qyljo1rSiL"
      },
      "source": [
        "# Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XQuBcxnrbTf5"
      },
      "outputs": [],
      "source": [
        "grid = [\n",
        "    [None, None, None, None, None, None, None, None, None, None, None],\n",
        "    [None, -0.1, -0.1, -0.1, -0.1, -0.1, -0.1, -0.1, None, +5.0, None],\n",
        "    [None, -0.1, None, None, None, None, None, None, None, -0.1, None],\n",
        "    [None, -0.1, -0.1, -0.1, -0.1, -0.1, -0.1, -0.1, -0.1, -0.1, None],\n",
        "    [None, -0.1, None, None, None, None, None, None, None, None, None],\n",
        "    [None, -0.1, None, -0.1, -0.1, -0.1, -0.1, -0.1, -0.1, -0.1, None],\n",
        "    [None, -0.1, None, None, None, None, None, -0.1, -0.1, -0.1, None],\n",
        "    [None, -0.1, -0.1, -0.1, -0.1, -0.1, -0.1, -0.1, -0.1, -0.1, None],\n",
        "    [None, None, None, None, None, -0.1, None, -0.1, -0.1, -0.1, None],\n",
        "    [None, -5.0, -0.1, -0.1, -0.1, -0.1, None, -0.1, -0.1, -0.1, None],\n",
        "    [None, None, None, None, None, None, None, None, None, None, None]\n",
        "]\n",
        "terminals = [(9, 9)]\n",
        "maze = GridMDP(grid, terminals)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6ceKmrYrcZe"
      },
      "source": [
        "# Q-Values and Best Policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tUQz01ZhdAj-"
      },
      "outputs": [],
      "source": [
        "def q_values(mdp, s, V):\n",
        "    res = [sum(p*V[si] for p, si in mdp.T(s, a))\n",
        "            for a in mdp.actions(s)]\n",
        "    return res\n",
        "\n",
        "\n",
        "def best_policy(mdp, V):\n",
        "    \"\"\"Given an MDP and a utility function U, determine the best policy,\n",
        "    as a mapping from state to action.\"\"\"\n",
        "    pi = {}\n",
        "    for s in mdp.states:\n",
        "        if s in mdp.terminals:  # Skip terminal states.\n",
        "            pi[s] = None\n",
        "            continue\n",
        "        qs = q_values(mdp, s, V)\n",
        "        pi[s]=  mdp.actions(s)[qs.index(max(qs))]\n",
        "    return pi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9_lp8MErj-6"
      },
      "source": [
        "# Value Iteration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UOf-pcruis50"
      },
      "outputs": [],
      "source": [
        "def value_iteration(self, iterations=20, epsilon=1e-3):\n",
        "    \"\"\"\n",
        "    Perform value iteration algorithm to solve the MDP.\n",
        "\n",
        "    Args:\n",
        "        iterations (int): Number of iterations.\n",
        "    Returns:\n",
        "        dict: Mapping from state to value.\n",
        "    \"\"\"\n",
        "\n",
        "    V = {s: 0 for s in self.states}\n",
        "    for _ in range(iterations):\n",
        "        _V = V.copy()\n",
        "        delta = 0\n",
        "\n",
        "        for s in self.states:\n",
        "            V[s] = self.R(s) + self.gamma * max(q_values(self, s, V))\n",
        "            delta = max(delta, abs(_V[s]-V[s]))\n",
        "\n",
        "        if delta <= epsilon * (1 - self.gamma) / self.gamma:\n",
        "            break\n",
        "    return V"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HSkw8mArqfG"
      },
      "source": [
        "# Run!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wo6iDZsWiVLZ",
        "outputId": "e6d05c7b-d874-4fbb-a5f9-15ee69c0b384"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#################################\n",
            "### ∨  <  <  <  <  <  < ### G ###\n",
            "### ∨ ##################### ∧ ###\n",
            "### >  >  >  >  >  >  >  >  ∧ ###\n",
            "### ∧ ###########################\n",
            "### ∧ ### >  >  >  >  ∨  <  < ###\n",
            "### ∧ ############### ∨  <  < ###\n",
            "### ∧  <  <  <  <  <  <  <  < ###\n",
            "############### ∧ ### ∧  <  < ###\n",
            "### >  >  >  >  ∧ ### ∧  <  < ###\n",
            "#################################\n"
          ]
        }
      ],
      "source": [
        "V = value_iteration(maze)\n",
        "pi = best_policy(maze, V)\n",
        "print_table(maze.to_arrows(pi))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5AaInvl3ubfH"
      },
      "source": [
        "# Policy Iteration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nQjAf9rSkMxj"
      },
      "outputs": [],
      "source": [
        "def policy_evaluation(mdp, pi, V, k=20):\n",
        "    \"\"\"Return an updated utility mapping V from each state in the MDP to its\n",
        "    utility, using an approximation (modified policy iteration).\"\"\"\n",
        "    for i in range(k):\n",
        "        for s in mdp.states:\n",
        "            V[s] = mdp.R(s) + mdp.gamma*sum(p*V[si] for p, si in mdp.T(s, pi[s]))\n",
        "    return V\n",
        "\n",
        "\n",
        "def policy_iteration(mdp, iterations=10):\n",
        "\n",
        "    import random\n",
        "    V = {s: 0 for s in mdp.states}\n",
        "    pi = {s: random.choice(mdp.actions(s)) for s in mdp.states}\n",
        "\n",
        "    for _ in range(iterations):\n",
        "        V = policy_evaluation(mdp, pi, V)\n",
        "        unchanged = True\n",
        "\n",
        "        for s in mdp.states:\n",
        "            qs = q_values(mdp, s, V)\n",
        "            q_max = max(qs)\n",
        "            a_max = mdp.actions(s)[qs.index(q_max)]\n",
        "\n",
        "            if q_max > sum(p*V[si] for p, si in mdp.T(s, pi[s])):\n",
        "                pi[s] = a_max\n",
        "                unchanged = False\n",
        "\n",
        "        if unchanged:\n",
        "            break\n",
        "\n",
        "    return pi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7vP2ReNu71D"
      },
      "source": [
        "# Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aN9aier_qjzR",
        "outputId": "f7ebe408-fdef-4028-a192-9d5b59c09896"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#################################\n",
            "### ∨  <  <  <  <  <  < ### G ###\n",
            "### ∨ ##################### ∧ ###\n",
            "### >  >  >  >  >  >  >  >  ∧ ###\n",
            "### ∧ ###########################\n",
            "### ∧ ### >  >  >  >  ∨  ∨  < ###\n",
            "### ∧ ############### ∨  <  < ###\n",
            "### ∧  <  <  <  <  <  <  <  < ###\n",
            "############### ∧ ### ∧  <  < ###\n",
            "### >  >  >  >  ∧ ### ∧  <  < ###\n",
            "#################################\n"
          ]
        }
      ],
      "source": [
        "pi = policy_iteration(maze)\n",
        "print_table(maze.to_arrows(pi))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y0OOWSpaqqvI"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
