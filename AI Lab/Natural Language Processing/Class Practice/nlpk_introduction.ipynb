{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenisation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokens are *the parts that make up a sentence*. They may or may not correspond exactly to words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "#import nltk\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first one parses sentences in smaller sentences, while the second tries to get down to the minimum number of words per token as possible. It is possible to have tokens bigger or smaller than a single word.  \n",
    "Moreover, they contain punctuation symbols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['It is dangerous to go alone.', 'Please take care.']\n"
     ]
    }
   ],
   "source": [
    "example_string = \"It is dangerous to go alone. Please take care.\"\n",
    "\n",
    "sent_res = sent_tokenize(example_string)\n",
    "print(sent_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see this relies heavily on punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['It', 'is', 'dangerous', 'to', 'go', 'alone', '.', 'Please', 'take', 'care', '.']\n"
     ]
    }
   ],
   "source": [
    "res = word_tokenize(example_string)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, instead, how this takes punctuation in consideration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /home/pachy/nltk_data...\n",
      "[nltk_data] Downloading package tagsets to /home/pachy/nltk_data...\n",
      "[nltk_data]   Unzipping help/tagsets.zip.\n",
      "[nltk_data] Downloading package punkt to /home/pachy/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/pachy/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/pachy/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package wordnet to /home/pachy/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('tagsets')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stopwords have little semantic meaning and it is useful to parse them out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In nltk jargon, *corpus* means text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['It', \"'s\", 'leviòsa', ',', 'not', 'leviosà']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "quote = \"It's leviòsa, not leviosà\"\n",
    "\n",
    "words_in_quote = word_tokenize(quote)\n",
    "words_in_quote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'s\", 'leviòsa', ',', 'leviosà']\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_list = []      #let's take out the stopwords\n",
    "\n",
    "for word in words_in_quote:\n",
    "    if word.casefold() not in stop_words:       #casefold makes the inclusion not case-sensitive\n",
    "        filtered_list.append(word)\n",
    "\n",
    "print(filtered_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming\n",
    "\n",
    "It is the preprocessing step consisting of the extraction of the root for a word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "quote = 'The crew of USS Discovery discovered many discoveries. Discovering is what explorers do.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'crew',\n",
       " 'of',\n",
       " 'uss',\n",
       " 'discoveri',\n",
       " 'discov',\n",
       " 'mani',\n",
       " 'discoveri',\n",
       " '.',\n",
       " 'discov',\n",
       " 'is',\n",
       " 'what',\n",
       " 'explor',\n",
       " 'do',\n",
       " '.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = word_tokenize(quote)\n",
    "stemmed_words = [stemmer.stem(word) for word in words]\n",
    "stemmed_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since PorterStemmer is from 1979, let's use something else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'crew',\n",
       " 'of',\n",
       " 'uss',\n",
       " 'discoveri',\n",
       " 'discov',\n",
       " 'mani',\n",
       " 'discoveri',\n",
       " '.',\n",
       " 'discov',\n",
       " 'is',\n",
       " 'what',\n",
       " 'explor',\n",
       " 'do',\n",
       " '.']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer(language = 'english')\n",
    "stemmed_words = [stemmer.stem(word) for word in words]\n",
    "stemmed_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Computer', 'NNP'),\n",
       " ('Science', 'NNP'),\n",
       " ('is', 'VBZ'),\n",
       " ('no', 'DT'),\n",
       " ('more', 'RBR'),\n",
       " ('about', 'IN'),\n",
       " ('computers', 'NNS'),\n",
       " ('than', 'IN'),\n",
       " ('astronomy', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('about', 'IN'),\n",
       " ('telescopes', 'NNS'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quote = 'Computer Science is no more about computers than astronomy is about telescopes.'\n",
    "\n",
    "tokens = word_tokenize(quote)\n",
    "tags = nltk.pos_tag(tokens)     #pos means \"parse of speech\"\n",
    "tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of these acronyms correponds to a part of the speech."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatizer tries to return the meaning of the word. It can be the root or something like the singular of a noun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'scarf'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "#a simple case\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "res = lemmatizer.lemmatize('scarves')\n",
    "\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'worst'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#a difficult case\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatizer.lemmatize('worst')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bad'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#a difficult case\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatizer.lemmatize('worst', pos = 'a')       #pos contains the part of the speech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See REGEX first!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_lab-aGYZY7pK-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
